# 8. 통합점 : 게이트웨이, 터널, 릴레이, 9. 웹 로봇

<br>

## 8장 게이트웨이, 터널, 릴레이

### 궁금한 부분
- 원격 프로시저 호출(remote procedure call)이란?
    - 별도의 원격 제어를 위한 코딩 없이 다른 주소 공간에서 함수나 프로시저를 실행할 수 있게 하는 프로세스 간 통신 기술
    - 네트워크를 통한 메시징을 수행하여 다른 주소 공간의 함수를 사용
    - 스프링에서 제공하지 않지만 유사한 기능 수행 방법으로 Spring Remoting, Spring Cloud Netflix - Feign가 있다고 함
- SOAP(Simple Object Access Protocol)과 XML(eXtensible Markup Language) 이란?
    - SOAP : 네트워크 상에서 서로 다른 시스템 간에 메시지를 교환하기 위한 프로토콜(흔히 HTTP, HTTPS, SMTP 등을 통해 XML 기반의 메시지를 컴퓨터 네트워크 상에서 교환하는 프로토콜)으로 주로 RestFul 방식과 비교되는 방법
    - XML : 다목적 마크업 언어로, 데이터를 구조화하고 표현하기 위한 형식을 정의하는 데 사용
    - SOAP은 XML을 기반으로 하며, 서로 다른 플랫폼과 언어 간의 상호 운용성을 제공하는 웹 서비스를 구현하는 데 사용됨
- HTTP는 HTTP 커넥션을 통하여 트래픽을 전송하는데 웹 터널에서는 HTTP TCP를 사용하되 HTTP가 아닌 트래픽을 전송한다는데 무슨 이야기 인지?
- 터널의 끝단 어느 부분이든 커넥션이 끊어지면 끊어진 곳으로부터 온 반대편으로 전달된다는 것은 서버에서 보냈지만 클라이언트 부분이 끊겨 있다면 다시 서버로 보내서 커넥션을 끊는다는 건지?
    - 터널의 어느 커넥션이 끊어지면 해당 전송은 다시 전송처로 돌아가거나 그자리에서 커넥션이 끊김
    - 전송하려던 메시지는 없어짐

<br>

### 게이트웨이
- 리소스와 애플리케이션을 연결하는 역할
    - 애플리케이션은 게이트웨이에게 요청을 처리해달라고 할 수 있고(HTTP 혹은 그 밖의 정의해 둔 인터페이스를 통해), 게이트웨이는 그에 응답할 수 있음
- 게이트웨이는 요청을 받고 응답을 보내는 포털 같이 동작하는데, 동적인 콘텐츠를 생성하거나 데이터베이스에 질의를 보낼 수 있음
- 클라이언트 측 프로토콜과 서버측 프로토콜을 빗금 (/)으로 구분하여 기술함
    - <클라이언트 프로토콜>/<서버 프로토콜>

<br>

### 프로토콜 게이트 웨이
- 프락시에 트래픽을 바로 보내는 것과 같이 게이트웨이에도 HTTP 트래픽을 바로 보낼 수 있음
- HTTP/*: 서버 측 웹 게이트웨이
    - 서버 측 웹 게이트웨이는 클라이언트로부터 HTTP 요청이 원 서버 영역으로 들어오는 시점에 클라이언트 측의 HTTP 요청을 외래 프로토콜로 전환함
- HTTP/FTP 게이트웨이 역할은 다음과 같은 일을 함
    - USER와 PASS 명령을 보내서 서버에 로그인
    - 서버에서 적절한 디렉터리로 변경하기 위해 CWD 명령을 내린다.
    - 다운로드 형식을 ASCII로 설정한다.
    - DTM으로 문서의 최근 수정 시간을 가져온다.
    - PASV로 서버에게 수동형 데이터 검색을 하겠다고 말한다.
    - RETR로 객체를 검색한다.
    - 제어 채널에서 반환된 포트로 FTP 서버에 데이터 커넥션을 맺는다. 데이터 채널이 열리는 대로, 객체가 게이트웨이로 전송
- HTTP/HTTPS: 서버 측 보안 게이트웨이
    - HTTP 클라이언트의 요청을 게이트웨이는 자동으로 사용자의 모든 세션을 암호화하여 보안 웹서버에게 전달함
    - HTTP 클라이언트 <-> HTTP/HTTPS 인바운드 보안 게이트웨이 <-> 보안 웹 서버
- HTTPS/HTTP: 클라이언트 측 보안 가속 게이트웨이
    - 해당 게이트웨이는 보안 HTTPS 트래픽을 받아서 복호화하고, 웹 서버로 보낼 일반 HTTP 요청을 만듬
    - 브라우저(SSL(HTTPS)를 통한 HTTP) <-> HTTPS/HTTP 보안 가속 게이트웨이 <-> 웹 서버

<br>

### 리소스 게이트웨이
- 게이트웨이의 가장 일반적인 형태인 애플리케이션 서버는 목적지 서버와 게이트웨이를 한 개의 서버로 결합함
- 애플리케이션 서버는 HTTP를 통해서 클라이언트와 통신하고 서버 측에 있는 애플리케이션 프로그램에 연결하는 서버 측 게이트웨이임
- CGI (공용 게이트웨이 인터페이스)
    - 최초의 서버 확장이자 지금까지도 가장 널리쓰이는 서버 확장임
    - 이는 웹에서 동적인 HTML, 신용카드 처리, DB 질의 등을 제공하는 데 사용
    - 초기에는 매 CGI요청마다 프로세스를 생성하기 때문에 부하가 컸지만 어느 정도 기술이 발전하면서 이런 성능 저하는 해결되었음

<br>

### 서버 확장 API
- 모듈을 HTTP와 직접 연결할 수 있는 강력한 인터페이스인 서버 확장 API를 제공함
- 확장 API는 프로그래머가 자신의 코드를 서버에 연결하거나 서버의 컴포넌트를 자신이 만든 것으로 교체할 수 있게 하였음
- FPSE(FrontPage Server Extension)
    - 프론트 페이지 클라이언트로부터 전송되는 원격 프로시져 호출 명령(Remote Procedure Call, RPC)을 인식할 수 있음
    - 이 명령은 HTTP에 편승하여 옴

<br>

### 애플리케이션 인터페이스와 웹 서비스
- 데이터를 교환할 때 두 어플리케이션의 프로토콜 인터페이스를 맞추는 것은 까다로운 이슈였음
    - HTTP 메시지 포맷으로는 제약이 있었기 때문
- HTTP 헤더에 XML을 사용하여 정보 교환을 하는 방식을 표준으로 삼았고 이를 SOAP(Simple Object Access Protocol)이라 함

<br>

### 터널
- 웹 터널은 HTTP 프로토콜을 지원하지 않는 애플리케이션에 HTTP 애플리케이션을 사용해 접근하는 방법을 제공
- CONNECT로 HTTP 터널 커넥션 맺기
    - 웹 터널은 HTTP의 CONNECT 메서드를 사용해 커넥션을 맺음
- CONNECT 메서드를 활용한 게이트웨이와 터널 연결하는 방법은 다음과 같음
    - 클라이언트는 게이트웨이여 터널을 연결하려고 CONNECT 요청을 보냄
    - TCP 커넥션은 게이트웨이 -> 웹 서버에 커넥션 요청으로 연결을 맺음
    - TCP 커넥션이 맺어지면 게이트웨이는 클라이언트에게 HTTP/1.0 200 Connection Established 응답 전송
    - 터널이 연결되고, HTTP 터널을 통해 전송된 클라이언트의 모든 데이터는 TCP 커넥션을 통해 웹 서버에 전달

```http
<!-- CONNECT 요청 --!>
CONNECT home.netscape.com:443 HTTP/1.0
User-Agent: Mozilla/4.0

<!-- CONNECT 응답 --!>
HTTP/1.0 200 Connection Established
Proxy-agent: Netscape-Proxy/1.2
```

<br>

### 데이터 터널링, 시간, 커넥션 관리
- 터널을 통해 전달되는 데이터는 게이트웨이에서 볼 수 없어서, 게이트웨이는 패킷의 순서나 흐름에 대한 어떤 가정도 할 수 없음
- 게이트웨이는 커넥션이 맺어지는 대로 헤더를 포함해서 읽어들인 모든 데이터를 서버에 전송해야 함
- 터널의 어느 부분이든 커넥션이 끊어지면, 그 곳으로부터 온 데이터는 반대편으로 전달되고, 그 다음 커넥션이 끊어졌던 터널의 끝단 반대편의 커넥션도 프락시에 의해서 끊어짐

<br>

### SSL 터널링
- 웹 터널은 방화벽을 통해서 암호화된 SSL 트래픽을 전달하려고 개발됨
- SSL 트래픽을 HTTP 커넥션으로 전송하여 80포트의 HTTP만을 허용하는 방화벽을 통과 시킴

<br>

### SSL 터널링 vs HTTP/HTTPS 게이트웨이
- HTTPS 프로토콜은 다른 프로토콜과 같은 방식으로 게이트웨이를 통과할 수 있음
- 게이트웨이가 FTP를 처리하는 방식과 같음
- HTTPS 프로토콜을 사용할 때의 단점은 다음과 같음
    - 클라이언트-게이트웨이 사이에는 보안이 적용되지 않은 일반 HTTP 커넥션이 맺어져 있음
    - 프락시가 인증을 담당하고 있기 때문에, 클라이언트는 원격 서버에 SSL 클라이언트 인증을 할 수 없음
    - 게이트웨이는 SSL을 완벽히 지원해야 함
- SSL 터널링을 사용하면
    - 프락시에 SSL을 구현할 필요가 없음
    - SSL 세션은 클라이언트가 생성한 요청과 목적지 웹 서버 간에 생성됨
    - 서버는 트랜잭션의 보안에는 관여하지 않고 암호화된 데이터를 그대로 터널링함

<br>

### 터널 인증
- HTTP의 다른 기능들은 터널과 함께 적절히 사용할 수 있음
- 프락시 인증 기능은, 클라이언트가 터널을 사용할 수 있는 권한을 검사하는 용도로 터널에서 사용할 수 있음

<br>

### 터널 보안에 대한 고려 사항
- 터널 게이트웨이는 통신하고 있는 프로토콜이 터널을 올바른 용도로 사용하고 있는지 검증할 방법이 없음
- 터널의 오용을 최소화하기 위해서, 게이트웨이는 HTTPS 전용 포트인 443같이 잘 알려진 특정 포트만을 터널링할 수 있게 허용해야 함

<br>

### 릴레이
- HTTP 명세를 완전히 준수하지 않는 간단한 HTTP 프락시
- HTTP 릴레이는 HTTP 요청과 응답을 중계하고 중간에서 요청을 수정하거나 응답을 필터링하지 않고 온전히 요청에 대한 응답을 전달만 함
    - 일반적으로 프록시보다 덜 복잡하며, 단순히 클라이언트와 서버 간의 연결을 중개하는 역할만 수행
    - 커넥션을 맺기 위한 HTTP 통신을 한 다음, 바이트를 전달함
- HTTP 릴레이는 요청과 응답을 변경하지 않고 전달하기 때문에 캐싱이나 보안 기능은 프록시보다 제한적

<br>

---

<br>

## 9장 웹 로봇

### 궁금한 부분
- 책에서 나온 예시 외의 크롤러를 사용하는 예시가 있을지?
- 웹 로봇이 80포트와 443포트로 같은 URL을 읽었을때 각각 저장하는지?
    - 로봇은 프로토콜과 포트 번호를 구분하여 저장하지는 않으므로 동일한 페이지로 저장한다고함
    - 하지만 저장 방식이나 로봇의 구현 방식에 따라 다를 수도 있음
- 웹 로봇의 루프를 피하기 위해 깊이 우선이 아닌 너비 우선이 좋은 이유는?
- 웹 로봇은 아직도 주로 요구사항이 적은 HTTP/1.0 으로 요청을 보내는지?
    - 아직까지 HTTP/1.0을 사용하는 로봇도 있지만 현재는 HTTP/1.1이나 HTTP/2.0을 주로 사용하다고 함
- 서버 요청 테스팅을 웹 로봇으로 서버 가용성을 체크해볼 수도 있지 않을까?
    - 일반적인 방법이라고 함
    - ex) Apache JMeter, Gatling, Selenium, Pingdom
- 현재의 로봇 차단 표준 버전
    - 여전히 1.0 을 주로 사용하며 일반적인 확장 방법으로 'robots meta'태그와 'Robots.txt Specification'을 사용한다고 함

```
- HTML 파일 내의 robots meta 태그
<meta name="robots" content="noindex, follow">

- Robots.txt Specification 예시
User-agent: *
Crawl-delay: 10
Allow: /public/
Noindex: /private/
```

<br>

### 웹 로봇
- 사람과의 상호작용 없이 연속된 웹 트랜잭션을 자동으로 수행하는 소프트웨어 프로그램
- 그 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불림
    - 주식시장 서버에서 매분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
    - 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇. 이것들은 웹을 떠돌면서 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록
    - 검색 데이터 베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
    - 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹페이지를 수집하는 가격 비교 로봇

<br>

### 1. 크롤러와 크롤링
- 웹 크롤러는 먼저 웹페이지를 한 개 가져오고 가져온 페이지가 가르키는 모든 웹페이지를 가져오는 것을 반복하여 가져오는 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
    - 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부르는데, HTML 하이퍼링크들로 만들어진 웹을 따라 '기어다니기(crawl)' 때문
- 웹 크롤링은 크롤링은 웹 크롤러가 웹 페이지를 방문하고 데이터를 수집하는 과정
- 추가로 웹 스크래핑(Web scraping)이란 크롤링보다는 좁은 범위의 데이터 수집에 주로 사용되는데 원하는 자료만들 가져와서 모으는 것을 의미

<br>

### 루트 집합
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 부름
- 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성됨

<br>

### 링크 추출과 상대 링크 정상화
- 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가 해야함
- 새 링크를 발견함에 따라 이 목록은 급속히 확장됨
- 크롤러는 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있음

<br>

### 순환 피하기
- 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야함

<br>

### 루프와 중복
- 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있음
- 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됨
- 크롤러는 많은 수의 중복된 페이지들을 가져오게 되는데 자신을 쓸모없게 만드는 중복된 콘텐츠로 넘쳐나게 될 것임

<br>

### 빵 부스러기의 흔적
- 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음. 대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법은 다음과 같음
1. 트리와 해시 테이블
    - 복잡한 로봇들은 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용
2. 느슨한 존재 비트맵
    - 공간 사용을 최소화하기 위해 존재 비트 배열과 같은 느슨한 자료구조를 사용
3. 체크 포인트
    - 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인
4. 파티셔닝
    - 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장(farm)을 이용함
    - 각 로봇엔 URL들의 특정 한 부분이 할당되어 그에 대한 책임을 갖음

<br>

### 별칭과 로봇 순환
- 한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있음

<br>

### URL 정규화하기
- 대부분의 웹 로봇은 URL들을 표준 형식으로 '정규화' 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도함
- 로봇은 다음과 같은 방식으로 모든 URL을 정규화된 형식으로 변환할 수 있음
    - 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가한다.
    - 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
    - \# 태그들을 제거한다.

<br>

### 파일 시스템 링크 순환
- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있음

<br>

### 동적 가상 웹 공간
- 웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만들 수 있음

<br>

### 루프와 중복 피하기
- 모든 순환을 피하는 완벽한 방법은 없으며 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 함
- 웹은 로봇이 문제를 일으킬 가능성으로 가득 차 있다. 이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같음
1. URL 정규화
    - URL을 표준 형태로 변환하여 중복된 URL 생기는 것을 일부 회피
2. 너비 우선 크롤링
    - 순환의 영향을 최소화함. 깊이 우선으로 운용하면 순환을 마주치면 빠져나올 수 없음
3. 스로틀링
    - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
4. URL 크기 제한
    - 로봇은 일정 길이를 넘는 URL의 크롤링을 거부할 수 있음. 순환으로 URL이 길어지면 길이 제한으로 중단될 것
5. URL/사이트 블랙리스트
    - 문제를 일으키는 사이트나 URL이 발견되면 블랙리스트에 추가하여 이후 해당 사이트를 피할 수 있음. 사람이 직접 추가가 필요
6. 패턴 발견
    - 파일 시스템의 심벌릭 링크 순환 같은 오설정들은 일정 패턴을 따르는 경향이 있는데 이러한 패턴의 URL 크롤링 거절
7. 콘텐츠 지문:
    - 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산. 하지만 어떤 웹 서버는 동적으로 페이지가 수정되어 체크섬 중복 감지 방해될 수 있음
8. 사람의 모니터링
    - 웹 로봇은 언제나 잘못될 수 있음. 사람의 모니터링에 의존하게 됨

<br>

### 2. 로봇의 HTTP
- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않고 그들 또한 HTTP 명세의 규칙을 지켜야함
- HTTP 요청을 만들고 스스로를 HTTP/1.1 클라이언트라고 광고하는 로봇은 적절한 HTTP 요청 헤더를 사용해야함

<br>

### 요청 헤더 식별하기
- 로봇 개발자들이 구현을 하도록 권장되는 기본적인 신원 식별 헤더들에는 다음과 같은 것이 있음
    - User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
    - From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
    - Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
    - Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공

<br>

### 조건부 요청
- 때때로 로봇들은 극악한 양의 요청을 시도하는데 이러한 검색하는 콘텐츠 양을 줄이는 것은 상당히 의미 있는일
    - 수십억 개의 웹페이지를 다운 받을 수 있는 검색엔진 로봇 같은 경우 오직 변경되었을 때만 콘텐츠를 가져오도록하는 것은 의미가 있음
- 이렇게 로봇 중의 몇몇은 시간이나 엔터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현

<br>

### 응답 다루기
- 대다수의 로봇들은 주 관심사가 단순히 Get메서드로 콘텐츠를 요청해서 가져오는 것이지만 특정 기능을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP응답을 다룰 줄 알 필요가 있음
- 상태 코드
    - 일반적으로 로봇들은 최소한 일반적인 상태코드나 예상할 수 있는 상태코드를 다룰 수 있어야함
    - 몇몇 서버는 언제나 항상 적절한 에러 코드를 반환하지는 않음
- 엔터티
    - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있음
    - 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보

<br>

### User-Agent 타기팅
- 웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고, 그 로봇들로부터의 요청을 예상해야함
- 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함

<br>

### 3. 부적절하게 동작하는 로봇들
- 로봇들이 저지르는 실수들은 다음과 같음
1. 폭주하는 로봇
    - 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP요청을 만들 수 있음
    - 만약 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있음
2. 오래된 URL
    - 로봇은 URL의 목록을 방문하고 목록이 오래되었을 경우 존재하지 않은 URL에 대한 요청을 보낼 수 있음
3. 길고 잘못된 URL
    - URL이 길다면 웹 서버의 처리 능력에 영향을 주고 고장을 일으킬 수도 있음
4. 호기심이 지나친 로봇
    - 로봇은 많은 양의 데이터를 검색하므로 몇몇 사이트 구현자들이 인터넷을 통해 접근 가능하리라 생각하지 못했던 민감한 데이터에 접근될 수도 있음
5. 동적 게이트웨이 접근
    - 로봇들이 접근하고 있는 것에 잘 알고 있는 것이 아니므로 게이트웨이 애플리케이션의 콘텐츠에 대한 URl 요청을 할 수도 있음

<br>

### 4. 로봇 차단하기
- 로봇에 대한 웹 사이트 접근이 유발할 수 있는 문제가 많음
- 이러한 문제에 대비하여 서버의 문서 루트에 robots.txt 같은 선택적인 파일을 제공하여 웹 로봇의 요청에 접근 가능 여부를 작성함
- 로봇 차단 표준은 0, 1.0, 2.0이 존재하는데 현재까지도 1.0 버전을 사용함

<br>

### 웹 사이트와 robots.txt 파일들
- 웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야함
- 웹 로봇이 요청을 보낼 때 우선 HTTP GET메서드를 이용해 robots.txt 리소스를 가져오고 응답코드를 받아옴
- 로봇이 웹 사이트에 robots.txt가 있는지 모르기 때문에 응답코드에 따라 동작
    - 서버가 성공 (2XX) : 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고 사이트로부터 무언가 가져올 때 해당 규칙을 따라야함
    - 리소스 존재하지 않음(404) : 활성화된 차단 규칙이 없으므로 제약없이 사이트 접근가능
    - 서버 접근 제한(401 or 403) : 로봇은 해당 사이트 접근은 완전히 제한되어 있음
    - 요청시도 일시 실패(503) : 로봇은 그 사이트의 리소스를 검색하는 것을 뒤로 미뤄야 함
    - 리다이렉션 필요(3XX) : 로봇은 리소스가 발견될 때까지 라다이렉트를 따라가야 함

<br>

### robots.txt 파일 포맷
- User-Agent
    - 각 로봇의 레코드는 하나 이상의 User-Agent줄로 시작하며 형식은 다음과 같음
- Disallow와 Allow
    - User-Agent 줄들 바로 다음에 위치
    - 특정 로봇에 대해 어떤 URL경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술함
- Disallow/Allow 접두 매칭(prefix matching)
    - Disallow/Allow 규칙이 어떤 경로에 적용되려면 그 경로의 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야함
    - User-Agent와 다르게 '*' 대신 빈칸을 사용하여 모든 문자열에 매치시킬수 있음
    - 규칙 경로나 URL 경로의 임의의 이스케이핑된 문자들(%XX)은 비교 전에 원래대로 복원됨 ('/'를 의미하는 %2F는 예외)
- 현재는 확장되어 Robots.txt Specification 요소들이 더 존재

<br>

### robots.txt 캐싱과 만료
- 캐시한 robots.txt는 바뀌었을 수 있으므로 로봇은 주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 함

<br>

### HTML 로봇 제어 META 태그
- HTML 페이지 저자는 로봇이 개별 페이지에 접근하는 것을 제한하는 좀 더 직접적인 방법을 갖고 있음
- HTML 페이지 내부에 meta 태그를 사용하여 웹 로봇을 제어하는 방법
- 모든 로봇 META 태그는 name="robots" 속성을 포함함
- 로봇 META 지시자는 다음과 같음
    - NOINDEX : 로봇은 이 페이지는 처리하지 말고 무시
    - NOFOLLOW : 로봇은 이 페이지 링크 페이지를 크롤링하지 말것
    - INDEX : 로봇은 이 페이지의 콘텐츠를 인덱싱 해도 됨
    - FOLLOW : 로봇은 이 페이지 링크 페이지를 크롤링해도 됨
    - NOARCHIVE : 로봇은 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안됨
    - ALL : INDEX + FOLLOW
    - NONE : NOINDEX + NOFOLLOW

<br>

### 5. 로봇 에티켓
- 1993년 웹 로봇 커뮤니티의 개척자인 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인 목록을 작성하였음
- 그 조언 중 몇 가지는 구식이 되어버렸지만, 대다수는 아직도 상당히 유용하다고 함
1. 신원 식별
    - 로봇의 신원을 밝혀라
    - 기계의 신원을 밝혀라
    - 연락처를 밝혀라
2. 동작
    - 긴장하라
    - 대비하라
    - 감시와 로그
    - 배우고 조정하라
3. 스스로를 제한하라
    - URL을 필터링하라
    - 동적 URL를 필터링하라
    - Accept 관련 헤더로 필터링
    - robots.txt에 따르라
    - 스스로를 억제하라
4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
    - 모든 응답 코드 다루기
    - URL 정규화 하기
    - 적극적으로 순환 피하기
    - 함정을 감시하라
    - 블랙리스트를 관리하라
5. 확장성
    - 공간 이해하기
    - 대역폭 이해하기
    - 시간 이해하기
    - 분할 정복
6. 신뢰성
    - 철저하게 테스트하라
    - 체크포인트
    - 실패에 대한 유연성
7. 소통
    - 준비하라
    - 이해하라
    - 즉각 대응하라

<br>

### 6. 검색 엔진
- 웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색 엔진
- 인터넷 검색엔진은 매우 많은 웹사이트를 사용자가 전 세계의 어떤 주제에 대한 문서라도 찾을 수 있게 함
- 웹 로봇의 주된 공급자인 검색엔진이 어떻게 동작하는지는 다음과 같음
1. 넓게 생각하라
    - 대규모 크롤러가 자신의 작업을 완료하려면 많은 장비를 똑똑하게 사용해서 요청을 병렬적로 수행할 수 있어야 할 것이라는 점은 명백
    - 하지만 그 규모 때문에, 웹 전체를 크롤링하는 것은 여전히 쉽지 않은 도전임
2. 현대적인 검색엔진의 아키텍쳐
    - 오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대해 '풀 텍스트 색인'이라고 하는 복잡한 로컬 데이터베이스를 생성함
    - 이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작함
    - 크롤링을 하는데 상당한 시간이 걸리지만 웹 페이지는 수시로 변경되므로 풀 텍스트 색인은 기껏 해봐야 웹의 특정 순간에 대한 스냅샷에 불과
3. 풀 텍스트 색인
    - 풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스임
    - 해당 풀 텍스트 색인은 각 단어를 포함한 문서들을 열거하고 키워드 일치 시 바로 해당 문서를 알려줌
    - 이 문서들은 색인이 생성된 후에는 검색할 필요가 없음
4. 질의 보내기
    - 사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은 HTML 폼을 사용자가 채워 넣고 브라우저가 그 폼을 HTTP GET 이나 POST 요청을 이요해서 게이트웨이로 보내는 방식임
    - 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용하는 표현식으로 변환함
5. 검색 결과를 정렬하고 보여주기
    - 질의의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 그 결과를 이용해 최종 사용자를 위한 결과 페이지를 즉석에서 만들어냄
6. 스푸핑
    - 검색 결과에서 더 높은 순위를 차지하고자 하는 바람은 검색 시스템과 게임으로 이어졌고, 검색엔진과 자신의 사이트를 눈에 띄게 할 방법을 찾고 있는 사람들과의 줄다리기를 만들어 냈음
    - 검색엔진과 로봇 구현자들은 속임수(검색 상위권을 위한 속임수)를 더 잘 잡아내기 위해 끊임없이 그들의 관련도 알고리즘을 수정해야 함

<br>

---

### 스터디 진행 중 나온 이야기
- 크롤러는 주로 파이선 라이브러리로 진행 하는 것 같음
- 웹 크롤러와 유사하지만 다른 웹 스크래핑도 존재
- HTTP 릴레이와 프록시의 차이
    - 해당 내용은 정리에 추가하였음

---

<br>

## 질의 문제

### 1. 기존 게이트웨이 통신과 터널 게이트웨이를 통한 통신의 차이점은?

<br>

<details>
<summary>정답</summary>

### 기존 게이트웨이 통신
- 클라이언트와 서버 간의 HTTP 통신을 중개하고 제어하는 역할으로 서로 다른 프로토콜을 중계하고 연결함
- 기존의 HTTP 통신은 일반적으로 텍스트 기반의 요청과 응답을 사용하여 HTTP 프로토콜의 특성상 헤더와 본문으로 구성되어 사용하는데 기존 게이트웨이는 HTTP 통신을 중개하고 제어하기 때문에 HTTP 프로토콜의 특성을 이해하고 해당 통신을 처리
    - 게이트웨이는 HTTP 통신의 요청과 응답을 이해하고 처리하기 위해 헤더와 본문을 확인하고 분석
    - HTTP 요청을 해석하고, 이에 따라 적절한 동작을 수행하여 처리함

<br>

### 터널 게이트 웨이 통신
- 터널 게이트웨이는 HTTP 프로토콜을 사용하지만 HTTP가 아닌 트래픽을 전송하는데 사용됨
    - 데이터에 상관없이 HTTP 에서의 TCP 연결을 통해 클라이언트와 서버 간의 통신을 터널링하여, 클라이언트나 서버 상관없이 보내는 모든 데이터를 전달함
    - HTTP 프로토콜 자체에 대한 이해나 해석은 필요하지 않음 (HTTP 헤더, 응답코드에 대한 처리 하지 않음)
    - 터널 게이트웨이를 사용하는 경우, 중간에 있는 게이트웨이들은 터널을 통해 데이터를 전달하기만 하고 내용을 확인할 수 없음
- 단순히 클라이언트와 서버 사이에 데이터를 전달하는 역할을 수행하므로 헤더 같은 프로토콜 레벨을 처리할 필요가 없음
    - 터널 게이트웨이는 데이터에 대해 처리하지 않으므로 데이터가 중간에 변경되지 않음을 보장함

<br>

</details>

<br>

### 2. 웹 로봇의 루프를 피하기 위해 깊이 우선 방식이 아닌 너비 우선 방식이 좋은 이유는?

<br>

<details>
<summary>정답</summary>

- 두 탐색방식 모두 그래프나 트리에서 탐색을 수행하는 알고리즘으로 웹 로봇이나 웹 크롤러는 웹 사이트를 탐색하는데 사용되므로 루프를 피하는 것에 좀 더 중점을 두어야 하므로
- 너비 우선 탐색 방식은 현재 노드의 모든 이웃을 먼저 탐색하는 방법으로 현재 레벨에 맞는 노드를 우선적으로 방문하므로 루프 발생 가능성이 줄어듬
    - 너비 우선 탐색을 사용하면 로봇이 특정 경로에서 루프에 빠져도 다른 경로에서 여전히 수많은 페이지를 수집할 수 있어 한 경로에서 루프에 빠져도 지속적으로 탐색이 가능하므로
- 깊이 우선 탐색 방식은 현재 경로를 따라 더 깊은 노드로 점차 탐색하기 때문에 더 빠르게 한 경로를 탐색하기에는 좋지만 주로 재귀적으로 동작하여 루프에 빠지기 쉬움
    - 재귀적으로 동작하기 때문에 우선 찾고 있는 경로에서 루프에 빠질 경우 다른 페이지를 탐색할 수 없고, 스택 오버플로우의 문제가 발생할 수도 있음

<br>

</details>

<br>

### Reference
- https://devhtak.github.io/http%20완벽%20가이드/2021/01/24/HTTP_Chap08.html
- https://velog.io/@jwun95/HTTP-%EC%99%84%EB%B2%BD-%EA%B0%80%EC%9D%B4%EB%93%9C-7%EC%9E%A5-%EC%9B%B9-%EB%A1%9C%EB%B4%87

<br>